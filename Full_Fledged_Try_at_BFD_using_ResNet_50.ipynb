{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nandodeomkar/BFD-using-ResNet50/blob/main/Full_Fledged_Try_at_BFD_using_ResNet_50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P03zeWzw2v6L"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install opencv-python\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J26WiTLosUR2"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Set the path to the directory containing the images\n",
        "dir_path = '/content/drive/MyDrive/Dataset/training'\n",
        "\n",
        "# Set the desired size for the images\n",
        "img_size = (224, 224)\n",
        "\n",
        "# Loop through the files in the directory\n",
        "for filename in os.listdir(dir_path):\n",
        "    # Check if the file is an image\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        # Load the image\n",
        "        img = cv2.imread(os.path.join(dir_path, filename))\n",
        "\n",
        "        # Resize the image to the desired size\n",
        "        resized_img = cv2.resize(img, img_size)\n",
        "\n",
        "        # Save the resized image with the same filename and extension\n",
        "        cv2.imwrite(os.path.join(dir_path, filename), resized_img)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Set the path to the directory containing the X-Ray images\n",
        "dir_path = '/content/drive/MyDrive/Dataset/training'\n",
        "\n",
        "# Set the desired size for the images\n",
        "img_size = (224, 224)\n",
        "\n",
        "# Initialize empty lists for the images and labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Loop through the files in the directory\n",
        "for filename in os.listdir(dir_path):\n",
        "    # Check if the file is an image\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        # Load the image\n",
        "        img = cv2.imread(os.path.join(dir_path, filename))\n",
        "\n",
        "        # Convert the image to grayscale\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Resize the image to the desired size\n",
        "        resized_img = cv2.resize(gray, img_size)\n",
        "\n",
        "        # Normalize the pixel values to be between 0 and 1\n",
        "        normalized_img = resized_img / 255.0\n",
        "\n",
        "        # Determine the label for the image based on the filename prefix\n",
        "        if filename.startswith('Y_'):\n",
        "            label = 1\n",
        "        elif filename.startswith('N_'):\n",
        "            label = 0\n",
        "        else:\n",
        "            label = -1\n",
        "\n",
        "        # Add the image and label to the lists\n",
        "        images.append(normalized_img)\n",
        "        labels.append(label)\n",
        "\n",
        "# Convert the lists of images and labels to NumPy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Add a channel dimension to the images array\n",
        "images = np.expand_dims(images, axis=-1)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation\n",
        "datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
        "\n",
        "# Fit the ImageDataGenerator to the training data\n",
        "datagen.fit(train_images)\n",
        "\n",
        "# Shuffle the training data\n",
        "idx = np.random.permutation(len(train_images))\n",
        "train_images, train_labels = train_images[idx], train_labels[idx]\n",
        "\n",
        "# One-hot encode the labels\n",
        "train_labels = to_categorical(train_labels)\n",
        "val_labels = to_categorical(val_labels)\n",
        "\n",
        "# Save the preprocessed data to disk\n",
        "np.save('train_images.npy', train_images)\n",
        "np.save('train_labels.npy', train_labels)\n",
        "np.save('val_images.npy', val_images)\n",
        "np.save('val_labels.npy', val_labels)\n"
      ],
      "metadata": {
        "id": "0cG7ZVdMrnXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96qmKdjOtxKq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "\n",
        "# Set the path to the directory containing the training images\n",
        "train_dir = '/path/to/train'\n",
        "\n",
        "# Set the desired size for the images\n",
        "img_size = (224, 224)\n",
        "\n",
        "# Set the batch size for training\n",
        "batch_size = 32\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation and preprocessing\n",
        "train_datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest', preprocessing_function=lambda x: x/255.0)\n",
        "\n",
        "# Load the ResNet50 model without the top layer\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "\n",
        "# Add a global average pooling layer and a dense output layer for binary classification\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the modified model\n",
        "model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "# Freeze the weights of the base ResNet50 model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Create a generator for the training data\n",
        "train_generator = train_datagen.flow_from_directory(train_dir, target_size=img_size, batch_size=batch_size, class_mode='binary')\n",
        "\n",
        "# Train the model for a few epochs\n",
        "model.fit(train_generator, epochs=1)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('bone_fracture_detection.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61lcAsxtyPAm"
      },
      "source": [
        "#Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5OnkXxLuMH6"
      },
      "source": [
        "Import ResNet 50 model and set it up for Bone Fracture Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P818G7TuXVg"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set the number of classes for the bone fracture detection task\n",
        "num_classes = 2\n",
        "\n",
        "# Load the ResNet50 model pre-trained on ImageNet\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add a global average pooling layer and a dense output layer for the bone fracture detection task\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create a new model with the ResNet50 base and the new output layers\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers in the ResNet50 base to prevent them from being trained\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Load the training data and labels\n",
        "train_data = np.load('images_array.npy')\n",
        "train_labels = pd.read_csv('image_labels.csv')\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "train_labels = keras.utils.to_categorical(train_labels['label'], num_classes)\n",
        "\n",
        "# Compile the model with a categorical cross-entropy loss function and an Adam optimizer\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the training data and labels\n",
        "model.fit(train_data, train_labels, epochs=1, batch_size=32, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLlHWyLXx_BU"
      },
      "source": [
        "#Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6-K3uEKyBql"
      },
      "source": [
        "Input Image to check for Bone Fracture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KGqg12cwzDm"
      },
      "source": [
        "#Push Changes to GitHub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QibqLVRoxW6s"
      },
      "source": [
        "Clone GitHub Repo to GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtAE7sp0xbid"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/nandodeomkar/BFD-using-ResNet50 /content/drive/MyDrive/GitHub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrfrB-dQxdJk"
      },
      "source": [
        "Stage the Changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm4mIqSsxjMd"
      },
      "outputs": [],
      "source": [
        "!git add ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JhNLYtPxp09"
      },
      "source": [
        "Commit Changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXS0P1fa1LIC"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"omkar.nandode1@gmail.com\"\n",
        "!git config --global user.name \"NandodeOmkar\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIiF88kgxrcV"
      },
      "outputs": [],
      "source": [
        "!git commit -m \"Pseudo Code Arranges in the Project\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk0urLZXxsZF"
      },
      "source": [
        "Push Changes"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "176ihp5DuWKGNyWNCkUK3PuVN_pYUCRw6",
      "authorship_tag": "ABX9TyO8+c3gJYkWB3DRJy8XPiUp",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}